\documentclass[10pt,technote]{IEEEtran}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[toc, page]{appendix}
\usepackage{graphicx}
\usepackage{subcaption}

\title{Coursework 2: Representation and Distance Metrics Learning }
\author{Timothee Gathmann \textit{(\textbf{tlg15}, 01061046)}\\ Luka Lagator\textit{ (\textbf{ll5915, }01108413)}}

\begin{document}

\maketitle
\begin{abstract}
In the aim of retrieving the most correct nearest neighbours of a labelled data point, different distance metrics were evaluated, without any training. The Mahalanobis distance was then implemented without and with training its distance matrix, and ultimately kernelised to possibly improve the nearest neighbour score. The distance metric optimisation didn't significantly improve on the euclidean metric, as it seemed that the data was already optimally represented.

\end{abstract}



\section{Problem formulation}
The features $X \in \mathbb{R}^{D X N}$ are readily available, and consist of a set of samples $x_i \in \mathbb{R}^D, i = 1, 2, ..., N$ corresponding to $N$ pictures of pedestrians. Each sample is assigned a ground-truth label $l(x_i) \in \mathbb{N}$ identifying the individual on the picture. The features are divided in a training subset $T$, a query subset $Q$ and a gallery subset $G$. Our goal is to minimise the retrieval error when performing retrieval experiments with the K-Nearest Neighbour algorithm \cite{Cover1967} at different ranks ($R = 1, 2, ..., 10 $), with different distance metrics. For a distance metric $d(x_i, x_j)$, the nearest neighbour $x_j \in X$ of $x_i$ is defined as
\begin{equation}
    \begin{aligned}
    n_0(x_i) = x_j \\
d(x_i, x_j) \le d(x_i, x_n), \forall x_n \in X - x_j
    \end{aligned}
\end{equation}
The $k$ nearest neighbours of $x_i \in X$ are :
\begin{equation}
    \begin{aligned}
    n_k(x_i) = \lbrace x_j, n_{k-1}\rbrace 
    \\
    d(x_i, x_j) \le d(x_i, x_n) \forall x_n \in X - n_k(x_i)
    \end{aligned}
\end{equation}
We define the mAp as
\begin{equation}
\begin{aligned}
    mAp & =   \frac{1}{N_Q}\sum^{N_Q}_{i, x_i \in Q}  \frac{1}{tot(x_i)}\sum^{N_G}_{j, x_j \in G}\rho(x_i, x_j)\frac{\sum^{j}_z\rho(x_i, x_z)}{j} \\ 
    tot(x_i)& = \sum^{N_G}_j\rho(x_i, x_j)
    \end{aligned}
\end{equation}
Where 
\begin{equation}
    \rho(x_i, x_z) =
    \begin{cases}
        1 & l(x_i) = l(x_z)\\
        0 & l(x_i) \neq l(x_z)
    \end{cases}
\end{equation}

It can be seen that, to minimise the mAp error, the feature space has to be transformed such that all points of same label lie the closest to each other, and all points of different labels lie the furthest from each other.
We can formulate our problem as a Distance Metric Learning problem. We define the set $S = \lbrace x_i, x_j, ...\rbrace, \rho(x_i, x_j) = 1$ and the set $D = \lbrace x_i, x_j, ...\rbrace,  \rho(x_i, x_j) = 0$
Then our problem is to solve:

\begin{equation}
\max \sum_{(x_i, x_j) \in D} d(x_i, x_j)
\end{equation}
And
\begin{equation}
\min \sum_{(x_i, x_j) \in S} d(x_i, x_j) < k, k \in \mathbb{R^+}
\end{equation}
The optimisable parameters depend on which function $d(x_i, x_j)$ is chosen, as well as which $k$ was chosen.

\section{Baseline experiments}

\subsection{Standard metrics}
The following metrics were considered as a baseline, without any transforms applied to the features:
\begin{enumerate}
    \item First order Minkowski
    \item Euclidean distance
    \item Cross Correlation similarity
    \item Cosine similarity
    \item Bilinear similarity
\end{enumerate}

We retrieved the k-nearest neighbours of the query features with the gallery features according to each baseline metric and calculated mAp for k = 1-15. The results can be seen in the figure \ref{fig:baseline_map}. Although all metrics perform in a similar trends as the rank of the nearest neighbour algorithm is changed, the Minkowsky metrics of order 1 and 2 perform better, surpassing constantly the third best performing cosine similarity by 10 percentage points. Bilinear Similarity metric was computed with the identity matrix and yielded the exact same results as cosine similarity, implying that normalisation by the 2-norm product makes no difference.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Graphs/mAp_vs_rank_baseline.png}
    \caption{Baseline mAps calculated at different ranks on testing data}
    \label{fig:baseline_map}
\end{figure}
\subsection{K-means}

\section{Improved approach: Kernel/Non-kernel Mahalanobis distance}
Mahalanobis distance learning was chosen as an attempt to improve the baseline metric, by learning the feature transformation that would improve the Euclidean distance metric. This seemed sensible as Euclidean distance metric was second only to Minkowski metric for $p=1$, and we wanted to try and improve upon this baseline. 
Where this distance is defined by:
\begin{equation}
    d_A(x, y) = (x - y)^TA(x - y)
\end{equation}
and $A \in \mathbb{R}^{DXD}$, $A \ge 0$

\subsection{Without iterative training: choosing the right A matrix}
It was first explored what performance could be achieved by simply choosing a good A matrix and not not train it any further. Two options were compared:
\begin{enumerate}
    \item $A = I \rightarrow d_A(x, y) = ||x - y||_2^2$
    \item $A = cov(x \in T)^{-1}$
\end{enumerate}
Their respective performance on the test dataset are displayed in figure \ref{fig:maha_map}. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Graphs/maha_Map.png}
    \caption{mAps calculated at different ranks on the testing data. The metric being tested is the Mahalanobis distance with two different A matrices}
    \label{fig:maha_map}
\end{figure}

In the case $A = I$, the Malahanobis metric is the Euclidean distance as shown in figure \ref{fig:baseline_map}. In comparison, the covariance based Mahalanobis metric doesn't show promising result. One reason for that might be that the features are already strongly decorrelated, and the covariance matrix does not apply any linear transformation to the features other than scaling each feature by the inverse of its variance (only diagonal elements in the covariance matrix). 

\subsection{Learning by constrained optimisation}
Following this we attempted to learn the matrix A through constrained optimisation. The objective function to minimise was the following:

\begin{equation}
    \min_{A}\sum_{(x_i, x_j) \in S}{d_A (x_i, x_j)}
\end{equation}
s.t.
\begin{equation}
    \begin{aligned}
        d_A{(x_i, x_k)} - d_A (x_i, x_j) \ge 1 \forall(x_i, x_j, x_k) \in{\mathbb{R}}  \\
        A \ge 0 \\
        where (x_i, x_j) \in S, (x_i, x_k) \in D
    \end{aligned}
\end{equation}
The optimisation problem was implemented as: 
\begin{equation}
    \begin{aligned}
    \min_{J,\lambda} L \\
    L(A, \lambda)  = \sum_{d_A \in D_s}d_A - |\lambda|C \\
        C = \sum_{}d_A{(x_i, x_k)} - d_A (x_i, x_j) -1,  \forall(x_j, x_k)  \\ \text{at 100 randomly chosen i}
    \end{aligned}
\end{equation}

The first term is the sum of distances between similar points (denoted by set Ds). The second term forms the sum of randomly generated constraints parameterised by $\lambda$, as all the constraint calculations could not be held in memory during runtime. The parameters for optimisation are the elements of A, used for distance computation, and the parameter $\lambda$. The optimisation was tested on three cases; with no kernel, as well as with the Gaussian and Polynomial kernels. With kernel learning, the variance $\sigma$ and power $p$ were scalar parameters to be optimised as well. The matrix A was initialised as either training data covariance matrix or as an identity matrix. The parameters l was initialised to 1, and kernel parameters $\sigma$ and $p$ to 30 and 0.1 respectively, when needed. $\sigma$ was chosen by inspection to prevent extreme values ([0,1]) as starting values for the kernel. Similarly $p$ was chosen to prevent tendency to $\inf$ of kernel function values.

Figure \ref{fig:comparison_init} shows the results of the three cases for two different initialisations of matrix A. We can see that the polynomial kernel does not perform well. In comparison, using no kernel or Gaussian kernel yields better results, where  the Gaussian kernel tends to do a bit better.
All the mAp for the testing data was calculated by kNN retrieval for $k = 10$.
The best result was achieved by the Gaussian kernel learning when the A matrix was initialised as I.

\begin{figure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Graphs/I_init_comparison.png}
    
    
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Graphs/cov_init_comparison.png}
\end{subfigure}
\caption{Evolution of mAp with the number of optimisation iterations, for the Mahalanobis distance initialised with the identity matrix (top) and the covariance matrix (bottom). The variants with no kernel, RBF kernel and polynomial kernel are shown.}
\label{fig:comparison_init}
\end{figure}

Figure \ref{fig:diff_matrix} and figure \ref{fig:diff_matrix_cov} show the difference between the matrix A (using Gaussian kernel) after and befeore training, for the two different initialisations of A. In both cases, there is no tendency to introduce any significant correlation between different features while learning, only diagonal weights show noticeable changes, where features are independently scaled. In the case where we start with the identity matrix I (only case that yields any improvement) even the diagonal terms change very little as we try to optimise our objective function. This means that all the features are already represented in the optimal domain for the Euclidean distance metric.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Graphs/A_matrix_diff.png}
    \caption{Difference of the RBF kernel Mahalanobis distance matrix after and before training, initialised as an identity matrix}
    \label{fig:diff_matrix}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Graphs/A_matrix_diff_cov.png}
    \caption{Difference of the RBF kernel Mahalanobis distance matrix after and before training, initialised as a covariance matrix}
    \label{fig:diff_matrix_cov}
\end{figure}

If we consider the cosine similarity (or biliniear similarity) as done in the section 2, we can see why the polynomial kernel might fail. As the inner product similarity measures do not yield great results, raising the inner product to a power $p$ is not the most promising way forward. The Gaussian kernel on the other hand is the exponential of Euclidean distance parameterised by $\sigma$. The maximum improvement upon the baseline was still quite low, at 1\% increase in mAp at k = 10, shown in table \ref{tab:my_label}.

\begin{table}
    \centering
    \begin{tabular}{c|c}
        Euclidean distance & RBF Kernel Mahalanobis distance \\
        50.64\% & 51.64 \%
    \end{tabular}
    \caption{Comparison of the mAp of the euclidean distance and our improved RBF kernel Mahalanobis distance initialised with an identity matrix}
    \label{tab:my_label}
\end{table}



\subsection{Alternative approaches}
We initially considered the random uniform initialisation of matrix A. The learning process yielded only worse results to the ones displayed above and at best, the mAp at k = 10 could reach 42\%. Introducing the off diagonal elements only impeded the performance and the optimisation.

We also tried a different approach when defining the constraints during the optimisation. We initialised the matrix to the identity matrix I, and optimised without the use of a kernel. Instead of taking random i at each training iteration and enforcing a different set of randomly chosen constraints, we tried a more deterministic approach. All the constraints were computed with the initial matrix $A=I$, and the constraints that were broken were found. In the following training only these constraints would be considered and recomputed at each iteration. This approach also meant that we can assign a different $\lambda$ for each constraint and implement a true Lagrangian optimisation over these constraints. This experiment yielded nearly same results as the previous method, but the algorithm converged to the achieved maximum much faster, as seen in figure \ref{fig:Lag_optim}. Due to a lack of computational power, it was impossible to implement this approach with kernels.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Graphs/no_kernel_Maha_I_newloss.png}
    \caption{Evolution of mAp with the number of optimisation iterations, using the Lagrangian optimisation and selected constraints}
    \label{fig:Lag_optim}
\end{figure}

Finally, we also considered learning the Bilinear similarity matrix, and using Bilinear similarity as the distance metric. We used the same loss and constraints as described in section B. It was not possible to learn the feature transform that would make the Bilinear similarity metric effective, shown in figure \ref{fig:}.



\appendices

\section{Optimisation of computations}

The deep learning platform PyTorch was used to perform most the large-scale matrix algebra as well as the optimisation. This platform allowed higher computation speeds by enabling computation graphs and making use of the parallel computing capabilities of our GPU.
The initial experiment was done by attempting to minimise the following objective function: \begin{equation}
    loss = l*\sum_{d \in S}d -  \sum_{d \in D}\sqrt{d}
\end{equation}
where set S contains all computed distances between similar data points and set D contains all the distances between dissimilar data points. This was a rudimentary implementation of the constrained optimisation problem defined as
\begin{equation}
    \max_{A}\sum_{(x_i, x_j) \in D}\sqrt{d_A (x_i, x_j)}
\end{equation}
s.t.
\begin{equation}
    \begin{aligned}
        c(A) = \sum_{(x_i, x_j) \in S}d_A (x_i, x_j) \le 1 \\
        A \ge 0
    \end{aligned}
\end{equation}
The A matrix was initialised as the 
inverse covariance matrix of the training features and decomposed by Cholesky decomposition to obtain the upper triangular matrix L. After transforming the training data points by L, Euclidian distance is computed on the transformed data points as follows:
\begin{equation}
    |Lx_i - Lx_j|_2^2 = \sum_{i}(Lx_i)^2 + \sum_{j}(Lx_j)^2 - 2(Lx_i)^T(Lx_j)
\end{equation}
The expansion on the right hand side ignores the square root operation in 2-norm computation. The square terms are first computed for all the training data points contained in the data matrix X and the last term is found as the matrix product of $X^TX$. This result gives the distance matrix of size $NxN$ where N is the number of training points. The diagonal is set to 0, as it should be when $x_i = x_j$, to stabilise the optimisation, since (due to computational precision) the summation of the first two terms slightly differs to the summation in the matrix product. 

\section{Code}
\subsection{Folder structure}

\begin{itemize}
    \item The file \textit{baseline.py} calculates the distances using all baseline metrics previously mentioned and stores their mAp results in a file.
    \item \textit{metrics.py} contains all the aforementioned metric functions that can calculate all the pairwise distances within a feature set in one call and take in a parameter if required (Mahalanobis, bilinear similarity). 
    \item \textit{optimisation.py} contains a class that wraps around a metric that uses a parameter and makes it trainable by defining an objective function and computing its gradient over the parameters. It contains as well the two losses that were used (first loss A and then loss C) and the main in which the metric is optimised. Within the main, you can select using the capital parameters how and which metric to optimise.

    \item \textit{data\_in\_out.py} contains all functions to load, save and display data, including the rank list
    \item \textit{kmeans.py} contains all the work done around kmeans, runs the clustering algorithm on query and gallery data and evaluates it in the end. The evaluation metric is not functional presently
    \item \textit{new\_optim.py} contains the work done to implement a new aforementioned loss (alternative approaches). It is not functional presently
\end{itemize}
\subsection{Optimising}
\begin{itemize}
    \item BATCHIFY, if set to True, runs the optimisation in batches specified in BATCH\_SIZE.
    \item KERNEL is the chosen kernel for the metric. Currently, all kernels are implemented for Mahalanobis, but no kernel is implemented for Bilinear Similarity
    \item RANK specifies the rank at which to calculate mAp
    \item SKIP\_STEP specifies how many times the objective funciton should be calculated and averaged before optimising. If BATCHIFY is True, the features used will be randomised
    \item FILENAME will specify the filename, imperative to change it to not overwrite previous data
    \item NUM\_ITER specifies the maximum number of iterations before stopping the algorithm
    \item INIT\_MODES specifies how the parameters should be initialised.'restore' will try to restore previously saved parameters
\end{itemize}



\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}
