@article{Cover1967,
abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error{\textless}tex{\textgreater}R{\textless}/tex{\textgreater}of such a rule must be at least as great as the Bayes probability of error{\textless}tex{\textgreater}R{\^{}}{\{}$\backslash$ast{\}}{\textless}/tex{\textgreater}--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the{\textless}tex{\textgreater}M{\textless}/tex{\textgreater}-category case that{\textless}tex{\textgreater}R{\^{}}{\{}$\backslash$ast{\}} $\backslash$leq R $\backslash$leq R{\^{}}{\{}$\backslash$ast{\}}(2 --MR{\^{}}{\{}$\backslash$ast{\}}/(M-1)){\textless}/tex{\textgreater}, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
author = {Cover, T and Hart, P},
doi = {10.1109/TIT.1967.1053964},
issn = {0018-9448 VO  - 13},
journal = {IEEE Transactions on Information Theory},
keywords = {Pattern classification},
number = {1},
pages = {21--27},
title = {{Nearest neighbor pattern classification}},
volume = {13},
year = {1967}
}
